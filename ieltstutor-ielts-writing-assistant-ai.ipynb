{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6aa1e6",
   "metadata": {
    "papermill": {
     "duration": 0.00755,
     "end_time": "2025-04-23T11:24:13.351047",
     "exception": false,
     "start_time": "2025-04-23T11:24:13.343497",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IELTSTutor - Empower your IELTS Writing\n",
    "IELTS Tutor is an AI agent designed to grade and provide detailed feedback on IELTS Academic Writing tasks 1 and 2 using LangGraph and Google's Gemini. The agent guides users through uploading a task description (image, PDF, or text), submitting a written response, and receiving structured feedback and band scores based on official IELTS band descriptors. It intelligently classifies the task (Task 1 vs. Task 2), identifies specific formats (e.g., bar chart, opinion essay), and provides task-specific instructions to guide the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c7d83",
   "metadata": {
    "papermill": {
     "duration": 0.006068,
     "end_time": "2025-04-23T11:24:13.363661",
     "exception": false,
     "start_time": "2025-04-23T11:24:13.357593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Significance of the project\n",
    "Recognizing that a minimum IELTS score of 6.5 is typically required for university admission and that academic writing presents the greatest challenge with an average score of only 6.0 (according to [ielts.org](https://ielts.org/researchers/our-research/test-statistics#Test_performance)), I intend to develop a free-to-use AI-powered IELTS Writing Assistant. This tool will offer comprehensive feedback and band score estimations to specifically aid students in enhancing their writing abilities for the IELTS exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28137810",
   "metadata": {
    "papermill": {
     "duration": 0.006005,
     "end_time": "2025-04-23T11:24:13.375824",
     "exception": false,
     "start_time": "2025-04-23T11:24:13.369819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overview of the project workflow\n",
    "* The user will be required to enter the task description. It can either be a task 1 or task 2 of any type; the model will automatically detect that. The task description can be in plain text, PDF, and/or images, as tasks 1 mostly contain some kind of graphs.\n",
    "* The user will upload their answer to the chat.\n",
    "* The agent will automatically detect whether the task description is for task 1 or task 2 and which type accordingly.\n",
    "* The agent will provide band score and detailed feedback based on official IELTS band descriptors and suggested structure for the required task type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7d83f",
   "metadata": {
    "papermill": {
     "duration": 0.005859,
     "end_time": "2025-04-23T11:24:13.387854",
     "exception": false,
     "start_time": "2025-04-23T11:24:13.381995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Details of the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453d05e",
   "metadata": {
    "papermill": {
     "duration": 0.005809,
     "end_time": "2025-04-23T11:24:13.399798",
     "exception": false,
     "start_time": "2025-04-23T11:24:13.393989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Get started\n",
    "Start by installing and importing the LangGraph SDK and LangChain support for the Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4f014c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:13.413486Z",
     "iopub.status.busy": "2025-04-23T11:24:13.413157Z",
     "iopub.status.idle": "2025-04-23T11:24:44.775060Z",
     "shell.execute_reply": "2025-04-23T11:24:44.773926Z"
    },
    "papermill": {
     "duration": 31.370821,
     "end_time": "2025-04-23T11:24:44.776720",
     "exception": false,
     "start_time": "2025-04-23T11:24:13.405899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping kfp as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Remove conflicting packages from the Kaggle base environment.\n",
    "!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n",
    "# Install langgraph and the packages used in this lab.\n",
    "!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea5905",
   "metadata": {
    "papermill": {
     "duration": 0.006853,
     "end_time": "2025-04-23T11:24:44.791294",
     "exception": false,
     "start_time": "2025-04-23T11:24:44.784441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Set up the API key\n",
    "The `GOOGLE_API_KEY` environment variable can be set to automatically configure the underlying API. This works for both the official Gemini Python SDK and for LangChain/LangGraph. \n",
    "\n",
    "To run the following cell, your API key must be stored it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n",
    "\n",
    "If you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n",
    "\n",
    "To make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b611ab97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:44.807155Z",
     "iopub.status.busy": "2025-04-23T11:24:44.806820Z",
     "iopub.status.idle": "2025-04-23T11:24:45.064971Z",
     "shell.execute_reply": "2025-04-23T11:24:45.064183Z"
    },
    "papermill": {
     "duration": 0.268165,
     "end_time": "2025-04-23T11:24:45.066551",
     "exception": false,
     "start_time": "2025-04-23T11:24:44.798386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc6df0",
   "metadata": {
    "papermill": {
     "duration": 0.006704,
     "end_time": "2025-04-23T11:24:45.080716",
     "exception": false,
     "start_time": "2025-04-23T11:24:45.074012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0c37e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:45.096459Z",
     "iopub.status.busy": "2025-04-23T11:24:45.096136Z",
     "iopub.status.idle": "2025-04-23T11:24:47.477712Z",
     "shell.execute_reply": "2025-04-23T11:24:47.476924Z"
    },
    "papermill": {
     "duration": 2.391771,
     "end_time": "2025-04-23T11:24:47.479458",
     "exception": false,
     "start_time": "2025-04-23T11:24:45.087687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "from typing import TypedDict, Optional, Any, Dict, List, Literal\n",
    "from PIL import Image\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a232a",
   "metadata": {
    "papermill": {
     "duration": 0.006921,
     "end_time": "2025-04-23T11:24:47.494277",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.487356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Define the state\n",
    "Defines the IeltsGraderState dictionary structure, which acts as the memory holding all information passed between steps in the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2db161e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.510066Z",
     "iopub.status.busy": "2025-04-23T11:24:47.509289Z",
     "iopub.status.idle": "2025-04-23T11:24:47.516578Z",
     "shell.execute_reply": "2025-04-23T11:24:47.515834Z"
    },
    "papermill": {
     "duration": 0.01664,
     "end_time": "2025-04-23T11:24:47.518024",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.501384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IeltsGraderState(TypedDict):\n",
    "    \"\"\"Represents the state of our IELTS grading graph.\"\"\"\n",
    "\n",
    "    # Raw Inputs (Set by interactive nodes)\n",
    "    task_description_input: Any # User-provided text or path\n",
    "    user_answer_input: Any    # User-provided text or path\n",
    "\n",
    "    # Processed Inputs\n",
    "    task_description_content: Optional[List[Dict[str, Any]]] # Langchain multimodal format (text/image data)\n",
    "    user_answer_text: Optional[str] # Plain text of the user's answer\n",
    "\n",
    "    # Task Identification Results\n",
    "    task_number: Optional[Literal[1, 2]] # Detected Task (1 or 2)\n",
    "    task_1_type: Optional[Literal[ # If Task 1, the specific type\n",
    "        'bar_chart', 'line_graph', 'pie_chart', 'table_chart',\n",
    "        'map', 'process_diagram', 'multiple_charts'\n",
    "    ]]\n",
    "    task_2_type: Optional[Literal[ # If Task 2, the specific type\n",
    "        'opinion', 'discussion', 'problem_solution',\n",
    "        'advantages_disadvantages', 'double_question'\n",
    "    ]]\n",
    "\n",
    "    # Intermediate Analysis / Tool Outputs\n",
    "    intermediate_results: Dict[str, Any] # e.g., storing visual summary\n",
    "\n",
    "    # Evaluation Output\n",
    "    evaluation_criteria: Optional[Dict[str, Dict[str, Any]]] # Detailed scores & feedback per criterion\n",
    "    overall_score: Optional[float] # Final calculated overall band score\n",
    "    final_feedback_report: Optional[str] # User-facing formatted report\n",
    "\n",
    "    # Control Flow & Error Handling\n",
    "    error_message: Optional[str] # Stores error messages if any step fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368583e",
   "metadata": {
    "papermill": {
     "duration": 0.006774,
     "end_time": "2025-04-23T11:24:47.531884",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.525110",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Official IELTS band descriptors and recommended structures\n",
    "Defines the core knowledge base: the detailed IELTS band descriptors (IELTS_RUBRICS) for Task 1 and Task 2 (including TA/TR, CC, LR, GRA) and the suggested paragraph structures (TASK_STRUCTURES) for different task types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0caeed1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.547471Z",
     "iopub.status.busy": "2025-04-23T11:24:47.547180Z",
     "iopub.status.idle": "2025-04-23T11:24:47.563736Z",
     "shell.execute_reply": "2025-04-23T11:24:47.562747Z"
    },
    "papermill": {
     "duration": 0.026438,
     "end_time": "2025-04-23T11:24:47.565085",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.538647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rubrics and Task Structures defined correctly.\n"
     ]
    }
   ],
   "source": [
    "# Define Task 1 Rubrics\n",
    "task1_rubrics = {\n",
    "    \"TA\": { # Task Achievement\n",
    "        9: \"\"\"All the requirements of the task are fully and appropriately satisfied.\n",
    "There may be extremely rare lapses in content.\"\"\",\n",
    "        8: \"\"\"The response covers all the requirements of the task appropriately, relevantly and sufficiently.\n",
    "(Academic) Key features are skilfully selected, and clearly presented, highlighted and illustrated.\n",
    "(General Training) All bullet points are clearly presented, and appropriately illustrated or extended.\n",
    "There may be occasional omissions or lapses in content.\"\"\",\n",
    "        7: \"\"\"The response covers the requirements of the task.\n",
    "The content is relevant and accurate - there may be a few omissions or lapses. The format is appropriate.\n",
    "(Academic) Key features which are selected are covered and clearly highlighted but could be more fully or more appropriately illustrated or extended.\n",
    "(Academic) It presents a clear overview, the data are appropriately categorised, and main trends or differences are identified.\n",
    "(General Training) All bullet points are covered and clearly highlighted but could be more fully or more appropriately illustrated or extended. It presents a clear purpose. The tone is consistent and appropriate to the task. Any lapses are minimal.\"\"\",\n",
    "        6: \"\"\"The response focuses on the requirements of the task and an appropriate format is used.\n",
    "(Academic) Key features which are selected are covered and adequately highlighted. A relevant overview is attempted. Information is appropriately selected and supported using figures/data.\n",
    "(General Training) All bullet points are covered and adequately highlighted. The purpose is generally clear. There may be minor inconsistencies in tone.\n",
    "Some irrelevant, inappropriate or inaccurate information may occur in areas of detail or when illustrating or extending the main points.\n",
    "Some details may be missing (or excessive) and further extension or illustration may be needed.\"\"\",\n",
    "        5: \"\"\"The response generally addresses the requirements of the task. The format may be inappropriate in places.\n",
    "(Academic) Key features which are selected are not adequately covered. The recounting of detail is mainly mechanical. There may be no data to support the description.\n",
    "(General Training) All bullet points are presented but one or more may not be adequately covered. The purpose may be unclear at times. The tone may be variable and sometimes inappropriate.\n",
    "There may be a tendency to focus on details (without referring to the bigger picture).\n",
    "The inclusion of irrelevant, inappropriate or inaccurate material in key areas detracts from the task achievement.\n",
    "There is limited detail when extending and illustrating the main points.\"\"\",\n",
    "        4: \"\"\"The response is an attempt to address the task.\n",
    "(Academic) Few key features have been selected.\n",
    "(General Training) Not all bullet points are presented.\n",
    "(General Training) The purpose of the letter is not clearly explained and may be confused. The tone may be inappropriate.\n",
    "The format may be inappropriate.\"\"\",\n",
    "        3: \"\"\"The response does not address the requirements of the task (possibly because of misunderstanding of the data/diagram/situation).\n",
    "Key features/bullet points which are presented may be largely irrelevant.\n",
    "Limited information is presented, and this may be used repetitively.\"\"\",\n",
    "        2: \"\"\"The content barely relates to the task.\"\"\",\n",
    "        1: \"\"\"The content is wholly unrelated to the task.\n",
    "Responses of 20 words or fewer are rated at Band 1.\n",
    "Any copied rubric must be discounted.\"\"\",\n",
    "        0: \"\"\"Should only be used where a candidate did not attend or attempt the question in any way, used a language other than English throughout, or where there is proof that a candidate's answer has been totally memorised.\"\"\"\n",
    "    },\n",
    "    \"CC\": { # Coherence and Cohesion\n",
    "        9: \"\"\"The message can be followed effortlessly.\n",
    "Cohesion is used in such a way that it very rarely attracts attention.\n",
    "Any lapses in coherence or cohesion are minimal.\n",
    "Paragraphing is skilfully managed.\"\"\",\n",
    "        8: \"\"\"The message can be followed with ease.\n",
    "Information and ideas are logically sequenced, and cohesion is well managed.\n",
    "Occasional lapses in coherence or cohesion may occur.\n",
    "Paragraphing is used sufficiently and appropriately.\"\"\",\n",
    "        7: \"\"\"Information and ideas are logically organised and there is a clear progression throughout the response. A few lapses may occur.\n",
    "A range of cohesive devices including reference and substitution is used flexibly but with some inaccuracies or some over/under use.\"\"\",\n",
    "        6: \"\"\"Information and ideas are generally arranged coherently and there is a clear overall progression.\n",
    "Cohesive devices are used to some good effect but cohesion within and/or between sentences may be faulty or mechanical due to misuse, overuse or omission.\n",
    "The use of reference and substitution may lack flexibility or clarity and result in some repetition or error.\"\"\",\n",
    "        5: \"\"\"Organisation is evident but is not wholly logical and there may be a lack of overall progression. Nevertheless, there is a sense of underlying coherence to the response.\n",
    "The relationship of ideas can be followed but the sentences are not fluently linked to each other.\n",
    "There may be limited/overuse of cohesive devices with some inaccuracy.\n",
    "The writing may be repetitive due to inadequate and/or inaccurate use of reference and substitution.\"\"\",\n",
    "        4: \"\"\"Information and ideas are evident but not arranged coherently, and there is no clear progression within the response.\n",
    "Relationships between ideas can be unclear and/or inadequately marked. There is some use of basic cohesive devices, which may be inaccurate or repetitive.\n",
    "There is inaccurate use or a lack of substitution or referencing.\"\"\",\n",
    "        3: \"\"\"There is no apparent logical organisation. Ideas are discernible but difficult to relate to each other.\n",
    "Minimal use of sequencers or cohesive devices. Those used do not necessarily indicate a logical relationship between ideas.\n",
    "There is difficulty in identifying referencing.\"\"\",\n",
    "        2: \"\"\"There is little relevant message, or the entire response may be off-topic.\n",
    "There is little evidence of control of organisational features.\"\"\",\n",
    "        1: \"\"\"The writing fails to communicate any message and appears to be by a virtual non-writer.\n",
    "Responses of 20 words or fewer are rated at Band 1.\"\"\",\n",
    "        0: \"\"\"Should only be used where a candidate did not attend or attempt the question in any way, used a language other than English throughout, or where there is proof that a candidate's answer has been totally memorised.\"\"\" # Assuming 0 applies similarly\n",
    "    },\n",
    "    \"LR\": { # Lexical Resource\n",
    "         9: \"\"\"Full flexibility and precise use are evident within the scope of the task.\n",
    "A wide range of vocabulary is used accurately and appropriately with very natural and sophisticated control of lexical features.\n",
    "Minor errors in spelling and word formation are extremely rare and have minimal impact on communication.\"\"\",\n",
    "         8: \"\"\"A wide resource is fluently and flexibly used to convey precise meanings within the scope of the task.\n",
    "There is skilful use of uncommon and/or idiomatic items when appropriate, despite occasional inaccuracies in word choice and collocation.\n",
    "Occasional errors in spelling and/or word formation may occur, but have minimal impact on communication.\"\"\",\n",
    "         7: \"\"\"The resource is sufficient to allow some flexibility and precision.\n",
    "There is some ability to use less common and/or idiomatic items.\n",
    "An awareness of style and collocation is evident, though inappropriacies occur.\n",
    "There are only a few errors in spelling and/or word formation, and they do not detract from overall clarity.\"\"\",\n",
    "         6: \"\"\"The resource is generally adequate and appropriate for the task.\n",
    "The meaning is generally clear in spite of a rather restricted range or a lack of precision in word choice.\n",
    "If the writer is a risk-taker, there will be a wider range of vocabulary used but higher degrees of inaccuracy or inappropriacy.\n",
    "There are some errors in spelling and/or word formation, but these do not impede communication.\"\"\",\n",
    "         5: \"\"\"The resource is limited but minimally adequate for the task.\n",
    "Simple vocabulary may be used accurately but the range does not permit much variation in expression.\n",
    "There may be frequent lapses in the appropriacy of word choice, and a lack of flexibility is apparent in frequent simplifications and/or repetitions.\n",
    "Errors in spelling and/or word formation may be noticeable and may cause some difficulty for the reader.\"\"\",\n",
    "         4: \"\"\"The resource is limited and inadequate for or unrelated to the task. Vocabulary is basic and may be used repetitively.\n",
    "There may be inappropriate use of lexical chunks (e.g. memorised phrases, formulaic language and/or language from the input material).\n",
    "Inappropriate word choice and/or errors in word formation and/or in spelling may impede meaning.\"\"\",\n",
    "         3: \"\"\"The resource is inadequate (which may be due to the response being significantly underlength).\n",
    "Possible over-dependence on input material or memorised language.\n",
    "Control of word choice and/or spelling is very limited, and errors predominate. These errors may severely impede meaning.\"\"\",\n",
    "         2: \"\"\"The resource is extremely limited with few recognisable strings, apart from memorised phrases.\n",
    "There is no apparent control of word formation and/or spelling.\"\"\",\n",
    "         1: \"\"\"No resource is apparent, except for a few isolated words.\n",
    "Responses of 20 words or fewer are rated at Band 1.\"\"\",\n",
    "         0: \"\"\"Should only be used where a candidate did not attend or attempt the question in any way, used a language other than English throughout, or where there is proof that a candidate's answer has been totally memorised.\"\"\"\n",
    "    },\n",
    "    \"GRA\": { # Grammatical Range and Accuracy\n",
    "         9: \"\"\"A wide range of structures within the scope of the task is used with full flexibility and control.\n",
    "Punctuation and grammar are used appropriately throughout.\n",
    "Minor errors are extremely rare and have minimal impact on communication.\"\"\",\n",
    "         8: \"\"\"A wide range of structures within the scope of the task is flexibly and accurately used.\n",
    "The majority of sentences are error-free, and punctuation is well managed.\n",
    "Occasional, non-systematic errors and inappropriacies occur, but have minimal impact on communication.\"\"\",\n",
    "         7: \"\"\"A variety of complex structures is used with some flexibility and accuracy.\n",
    "Grammar and punctuation are generally well controlled, and error-free sentences are frequent.\n",
    "A few errors in grammar may persist, but these do not impede communication.\"\"\",\n",
    "         6: \"\"\"A mix of simple and complex sentence forms is used but flexibility is limited.\n",
    "Examples of more complex structures are not marked by the same level of accuracy as in simple structures.\n",
    "Errors in grammar and punctuation occur, but rarely impede communication.\"\"\",\n",
    "         5: \"\"\"The range of structures is limited and rather repetitive.\n",
    "Although complex sentences are attempted, they tend to be faulty, and the greatest accuracy is achieved on simple sentences.\n",
    "Grammatical errors may be frequent and cause some difficulty for the reader.\n",
    "Punctuation may be faulty.\"\"\",\n",
    "         4: \"\"\"A very limited range of structures is used.\n",
    "Subordinate clauses are rare and simple sentences predominate.\n",
    "Some structures are produced accurately but grammatical errors are frequent and may impede meaning.\n",
    "Punctuation is often faulty or inadequate.\"\"\",\n",
    "         3: \"\"\"Sentence forms are attempted, but errors in grammar and punctuation predominate (except in memorised phrases or those taken from the input material). This prevents most meaning from coming through.\n",
    "Length may be insufficient to provide evidence of control of sentence forms.\"\"\",\n",
    "         2: \"\"\"There is little or no evidence of sentence forms (except in memorised phrases).\"\"\",\n",
    "         1: \"\"\"No rateable language is evident.\n",
    "Responses of 20 words or fewer are rated at Band 1.\"\"\",\n",
    "         0: \"\"\"Should only be used where a candidate did not attend or attempt the question in any way, used a language other than English throughout, or where there is proof that a candidate's answer has been totally memorised.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define Task 2 Rubrics, Reusing Task 1 definitions where appropriate\n",
    "task2_rubrics = {\n",
    "    \"TR\": { # Task Response\n",
    "        9: \"\"\"The prompt is appropriately addressed and explored in depth.\n",
    "A clear and fully developed position is presented which directly answers the question/s.\n",
    "Ideas are relevant, fully extended and well supported.\n",
    "Any lapses in content or support are extremely rare.\"\"\",\n",
    "        8: \"\"\"The prompt is appropriately and sufficiently addressed.\n",
    "A clear and well-developed position is presented in response to the question/s.\n",
    "Ideas are relevant, well extended and supported.\n",
    "There may be occasional omissions or lapses in content.\"\"\",\n",
    "        7: \"\"\"The main parts of the prompt are appropriately addressed.\n",
    "A clear and developed position is presented.\n",
    "Main ideas are extended and supported but there may be a tendency to over-generalise or there may be a lack of focus and precision in supporting ideas/material.\"\"\",\n",
    "        6: \"\"\"The main parts of the prompt are addressed (though some may be more fully covered than others). An appropriate format is used.\n",
    "A position is presented that is directly relevant to the prompt, although the conclusions drawn may be unclear, unjustified or repetitive.\n",
    "Main ideas are relevant, but some may be insufficiently developed or may lack clarity, while some supporting arguments and evidence may be less relevant or inadequate.\"\"\",\n",
    "        5: \"\"\"The main parts of the prompt are incompletely addressed. The format may be inappropriate in places.\n",
    "The writer expresses a position, but the development is not always clear.\n",
    "Some main ideas are put forward, but they are limited and are not sufficiently developed and/or there may be irrelevant detail.\n",
    "There may be some repetition.\"\"\",\n",
    "        4: \"\"\"The prompt is tackled in a minimal way, or the answer is tangential, possibly due to some misunderstanding of the prompt. The format may be inappropriate.\n",
    "A position is discernible, but the reader has to read carefully to find it.\n",
    "Main ideas are difficult to identify and such ideas that are identifiable may lack relevance, clarity and/or support.\n",
    "Large parts of the response may be repetitive.\"\"\",\n",
    "        3: \"\"\"No part of the prompt is adequately addressed, or the prompt has been misunderstood.\n",
    "No relevant position can be identified, and/or there is little direct response to the question/s.\n",
    "There are few ideas, and these may be irrelevant or insufficiently developed.\"\"\",\n",
    "        2: \"\"\"The content is barely related to the prompt.\n",
    "No position can be identified.\n",
    "There may be glimpses of one or two ideas without development.\"\"\",\n",
    "        1: \"\"\"The content is wholly unrelated to the prompt.\n",
    "Responses of 20 words or fewer are rated at Band 1.\n",
    "Any copied rubric must be discounted.\"\"\",\n",
    "        0: \"\"\"Should only be used where a candidate did not attend or attempt the question in any way, used a language other than English throughout, or where there is proof that a candidate's answer has been totally memorised.\"\"\"\n",
    "    },\n",
    "    # Reuse CC, LR, GRA from Task 1 as they are identical\n",
    "    \"CC\": task1_rubrics[\"CC\"],\n",
    "    \"LR\": task1_rubrics[\"LR\"],\n",
    "    \"GRA\": task1_rubrics[\"GRA\"]\n",
    "}\n",
    "\n",
    "# Combine them into the main IELTS_RUBRICS dictionary\n",
    "IELTS_RUBRICS = {\n",
    "    \"Task 1\": task1_rubrics,\n",
    "    \"Task 2\": task2_rubrics\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# TASK STRUCTURES Definition\n",
    "TASK_STRUCTURES = {\n",
    "    \"Task 1\": {\n",
    "        'bar_chart': \"1. Intro: Paraphrase task, mention chart type, axes, units, overall trend/highest/lowest. 2. Overview: Summarize main groupings or comparisons. 3. Detail Para 1: Describe significant categories/bars. 4. Detail Para 2: Describe other categories/bars, making comparisons.\",\n",
    "        'line_graph': \"1. Intro: Paraphrase task, mention graph type, axes, units, overall trend. 2. Overview: Summarize main trend(s) or key features (start/end points, peaks). 3. Detail Para 1: Describe trends for one/two lines or from start to mid-point. 4. Detail Para 2: Describe trends for remaining lines or from mid-point to end, making comparisons.\",\n",
    "        'pie_chart': \"1. Intro: Paraphrase task, mention chart type, what it represents overall. 2. Overview: Summarize largest/smallest segments, maybe combined categories. 3. Detail Para 1: Describe largest segments with data. 4. Detail Para 2: Describe smaller/other significant segments, making comparisons.\",\n",
    "        'table_chart': \"1. Intro: Paraphrase task, mention table content. 2. Overview: Summarize main patterns, highest/lowest values overall or by key category. 3. Detail Para 1: Describe data for key rows/columns. 4. Detail Para 2: Describe other significant data points, making comparisons.\",\n",
    "        'map': \"1. Intro: Paraphrase task, state map locations/time periods. 2. Overview: Summarize the main change(s) overall (e.g., more residential, industrialization). 3. Detail Para 1: Describe changes in one key area/time period (e.g., north side, first map). 4. Detail Para 2: Describe changes in another key area/time period, noting comparisons.\",\n",
    "        'process_diagram': \"1. Intro: Paraphrase task, state process name/purpose. 2. Overview: State number of stages, start/end points. 3. Detail Para 1: Describe initial stages sequentially using appropriate sequencing language. 4. Detail Para 2: Describe later stages sequentially until the end.\",\n",
    "        'multiple_charts': \"1. Intro: Paraphrase task, mention types and topics of charts. 2. Overview: Summarize main point from each chart OR the main relationship between them. 3. Detail Para 1: Describe key features of Chart 1. 4. Detail Para 2: Describe key features of Chart 2, linking to Chart 1 where relevant.\",\n",
    "        'unknown': \"Standard Task 1 structure: Intro (Paraphrase, mention visual type if possible), Overview (Summarize main trends/features), Detail Paragraphs (Describe specific data/elements with comparisons).\"\n",
    "    },\n",
    "    \"Task 2\": {\n",
    "        'opinion': \"1. Intro: Paraphrase question, clearly state your opinion (agree/disagree/extent). 2. Body Para 1: Main reason supporting your opinion + explanation/example. 3. Body Para 2: Second reason supporting your opinion + explanation/example. (Alternative: Discuss opposing view briefly then refute). 4. Conclusion: Summarize main points and restate opinion in different words.\",\n",
    "        'discussion': \"1. Intro: Paraphrase question, introduce both views, state your own opinion (if asked, or can be in conclusion). 2. Body Para 1: Discuss the first view + reasons/examples. 3. Body Para 2: Discuss the second view + reasons/examples. 4. Conclusion: Summarize both views, give a balanced concluding thought or state your clear opinion.\",\n",
    "        'problem_solution': \"1. Intro: Paraphrase topic, state the problem(s) and that solutions will be discussed. 2. Body Para 1: Explain the main problem(s) or cause(s) + effects/examples. (Can be 1-2 paragraphs). 3. Body Para 2: Propose solution(s) + explain how they work/why they are effective. (Can be 1-2 paragraphs). 4. Conclusion: Summarize problem(s) and solution(s), give final recommendation/outlook.\",\n",
    "        'advantages_disadvantages': \"1. Intro: Paraphrase topic, state that advantages and disadvantages will be discussed. (State your opinion if asked: 'Do advantages outweigh...?') 2. Body Para 1: Discuss advantage(s) + explanation/examples. 3. Body Para 2: Discuss disadvantage(s) + explanation/examples. 4. Conclusion: Summarize points, give balanced view or state opinion on whether advantages outweigh disadvantages (if asked).\",\n",
    "        'double_question': \"1. Intro: Paraphrase topic, briefly introduce both questions. 2. Body Para 1: Answer the first question + reasons/examples. 3. Body Para 2: Answer the second question + reasons/examples. 4. Conclusion: Briefly summarize answers to both questions.\",\n",
    "        'unknown': \"Standard Task 2 essay structure: Intro (Hook, Background, Thesis/Outline). Body Paragraphs (Topic sentence, Explanation, Example). Conclusion (Summarize, Restate thesis, Final thought).\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Rubrics and Task Structures defined correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b470e514",
   "metadata": {
    "papermill": {
     "duration": 0.006781,
     "end_time": "2025-04-23T11:24:47.579075",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.572294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### LLM initialization\n",
    "Sets up the connection to the Large Language Models. It initializes a standard text model (llm) for simpler tasks and a more powerful multimodal model (multimodal_llm) capable of processing images and handling the complex evaluation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b7e6269",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.594372Z",
     "iopub.status.busy": "2025-04-23T11:24:47.594077Z",
     "iopub.status.idle": "2025-04-23T11:24:47.689697Z",
     "shell.execute_reply": "2025-04-23T11:24:47.688752Z"
    },
    "papermill": {
     "duration": 0.105452,
     "end_time": "2025-04-23T11:24:47.691418",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.585966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the standard model for text-based tasks (classification, etc.)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,\n",
    "    request_options={\"timeout\": 120}\n",
    ")\n",
    "\n",
    "# Initialize the multimodal model for tasks involving images or complex evaluation\n",
    "multimodal_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.2,\n",
    "    request_options={\"timeout\": 300}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ba134",
   "metadata": {
    "papermill": {
     "duration": 0.006714,
     "end_time": "2025-04-23T11:24:47.706118",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.699404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Input and loading\n",
    "Contains the functions for the interactive part (ask_for_*) that prompt the user for input, and the processing functions (load_*) that take the user's input (text, file path for Image/PDF/TXT) and load/convert it into the format needed by the agent (text strings and base64 encoded image data). Handles basic file loading errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05b2b29a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.721437Z",
     "iopub.status.busy": "2025-04-23T11:24:47.721128Z",
     "iopub.status.idle": "2025-04-23T11:24:47.730558Z",
     "shell.execute_reply": "2025-04-23T11:24:47.729382Z"
    },
    "papermill": {
     "duration": 0.019234,
     "end_time": "2025-04-23T11:24:47.732369",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.713135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Input node functions defined.\n"
     ]
    }
   ],
   "source": [
    "def ask_for_task_description(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Prompts the user to input the task description (text, path to PDF, or path to image).\"\"\"\n",
    "    print(\"\\n--- Task Description Input ---\")\n",
    "    try:\n",
    "        user_input = input(\"➡️ Enter Task Description text OR full path to PDF/Image file (e.g., /kaggle/input/.../task1.pdf or /kaggle/working/chart.png): \\n\")\n",
    "        if not user_input or not user_input.strip():\n",
    "            return {\"error_message\": \"Task description input cannot be empty.\"}\n",
    "        print(f\"   Received task description input: '{user_input.strip()[:100]}...'\")\n",
    "        # Store the raw input; loading/validation happens in the next node\n",
    "        return {\"task_description_input\": user_input.strip(), \"error_message\": None}\n",
    "    except EOFError: # Handles script execution ending unexpectedly during input\n",
    "        return {\"error_message\": \"Input stream closed. Cannot get task description.\"}\n",
    "    except Exception as e:\n",
    "        return {\"error_message\": f\"Error getting task description input: {e}\"}\n",
    "\n",
    "def ask_for_user_answer(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Prompts the user to input their answer (text or path to .txt file).\"\"\"\n",
    "    # Check for errors from previous steps before proceeding\n",
    "    if state.get(\"error_message\"): return {\"error_message\": state[\"error_message\"]}\n",
    "\n",
    "    print(\"\\n--- User Answer Input ---\")\n",
    "    try:\n",
    "        user_input = input(\"➡️ Enter your Answer text OR full path to a .txt file containing your answer: \\n\")\n",
    "        if not user_input or not user_input.strip():\n",
    "            return {\"error_message\": \"User answer input cannot be empty.\"}\n",
    "        print(f\"   Received user answer input: '{user_input.strip()[:100]}...'\")\n",
    "        # Store the raw input\n",
    "        return {\"user_answer_input\": user_input.strip(), \"error_message\": None}\n",
    "    except EOFError:\n",
    "         return {\"error_message\": \"Input stream closed. Cannot get user answer.\"}\n",
    "    except Exception as e:\n",
    "        return {\"error_message\": f\"Error getting user answer input: {e}\"}\n",
    "print(\"✅ Input node functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aede6274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.749078Z",
     "iopub.status.busy": "2025-04-23T11:24:47.748616Z",
     "iopub.status.idle": "2025-04-23T11:24:47.771572Z",
     "shell.execute_reply": "2025-04-23T11:24:47.770697Z"
    },
    "papermill": {
     "duration": 0.033197,
     "end_time": "2025-04-23T11:24:47.773003",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.739806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loading node functions defined.\n"
     ]
    }
   ],
   "source": [
    "def load_task_description(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Loads and processes the task description input (text, PDF, image) into Langchain format.\"\"\"\n",
    "    print(\"--- Loading Task Description ---\")\n",
    "    if state.get(\"error_message\"): return {\"error_message\": state[\"error_message\"]} # Propagate previous errors\n",
    "\n",
    "    task_input = state.get('task_description_input')\n",
    "    content_list = [] # To store Langchain formatted content [{type: 'text', text: '...'}, {type: 'image_url', ...}]\n",
    "    error_message = None\n",
    "\n",
    "    if not task_input:\n",
    "         return {\"error_message\": \"Task description input missing in state.\"}\n",
    "\n",
    "    try:\n",
    "        # Check if the input string is a valid, existing file path\n",
    "        if isinstance(task_input, str) and os.path.exists(task_input) and os.path.isfile(task_input):\n",
    "            print(f\"   Processing file path: {task_input}\")\n",
    "            file_ext = os.path.splitext(task_input)[1].lower()\n",
    "\n",
    "            # Handle Images\n",
    "            if file_ext in ['.png', '.jpg', '.jpeg', '.webp', '.gif']:\n",
    "                try:\n",
    "                    print(f\"   Encoding image file ({file_ext})...\")\n",
    "                    image = Image.open(task_input)\n",
    "                    buffer = io.BytesIO()\n",
    "                    # Ensure image is in RGB format for broad compatibility\n",
    "                    if image.mode in ['RGBA', 'P', 'LA']:\n",
    "                        image = image.convert('RGB')\n",
    "                    image.save(buffer, format=\"JPEG\") # Standardize to JPEG\n",
    "                    encoded_string = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "                    # Add a text placeholder and the image data\n",
    "                    content_list.append({\"type\": \"text\", \"text\": f\"[Image Provided: {os.path.basename(task_input)}]\"})\n",
    "                    content_list.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_string}\"}})\n",
    "                    print(\"   Image encoded successfully.\")\n",
    "                except Exception as img_e:\n",
    "                    print(f\"   ⚠️ Error processing image file: {img_e}\")\n",
    "                    error_message = f\"Failed to load/encode image file '{os.path.basename(task_input)}': {img_e}\"\n",
    "                    # Add error text to content list for context\n",
    "                    content_list.append({\"type\": \"text\", \"text\": f\"[Error loading image: {error_message}]\"})\n",
    "\n",
    "            # Handle PDFs\n",
    "            elif file_ext == '.pdf':\n",
    "                try:\n",
    "                    print(\"   Extracting text from PDF...\")\n",
    "                    pdf_text = \"\"\n",
    "                    reader = pypdf.PdfReader(task_input)\n",
    "                    for page_num, page in enumerate(reader.pages):\n",
    "                        pdf_text += page.extract_text() or \"\" # Add text from each page\n",
    "                        if page_num < len(reader.pages) - 1:\n",
    "                             pdf_text += \"\\n\\n--- Page Break ---\\n\\n\" # Indicate page breaks\n",
    "                    if pdf_text.strip():\n",
    "                        content_list.append({\"type\": \"text\", \"text\": pdf_text})\n",
    "                        print(f\"   Extracted {len(reader.pages)} page(s) of text from PDF.\")\n",
    "                    else:\n",
    "                        print(\"   ⚠️ PDF contained no extractable text.\")\n",
    "                        content_list.append({\"type\": \"text\", \"text\": f\"[PDF file provided ({os.path.basename(task_input)}) but contained no extractable text]\"})\n",
    "                except Exception as pdf_e:\n",
    "                    print(f\"   ⚠️ Error processing PDF file: {pdf_e}\")\n",
    "                    error_message = f\"Failed to read PDF file '{os.path.basename(task_input)}': {pdf_e}\"\n",
    "                    content_list.append({\"type\": \"text\", \"text\": f\"[Error loading PDF: {error_message}]\"})\n",
    "\n",
    "            # Handle Plain Text Files\n",
    "            elif file_ext == '.txt':\n",
    "                 try:\n",
    "                     print(\"   Reading text file...\")\n",
    "                     with open(task_input, 'r', encoding='utf-8') as f:\n",
    "                         text_content = f.read()\n",
    "                         content_list.append({\"type\": \"text\", \"text\": text_content})\n",
    "                         print(f\"   Loaded text from '{os.path.basename(task_input)}'.\")\n",
    "                 except Exception as txt_e:\n",
    "                     print(f\"   ⚠️ Error reading text file: {txt_e}\")\n",
    "                     error_message = f\"Failed to read text file '{os.path.basename(task_input)}': {txt_e}\"\n",
    "                     content_list.append({\"type\": \"text\", \"text\": f\"[Error loading text file: {error_message}]\"})\n",
    "\n",
    "            # Unsupported file types\n",
    "            else:\n",
    "                error_message = f\"Unsupported file type provided for task description: '{file_ext}'\"\n",
    "                print(f\"   ⚠️ {error_message}\")\n",
    "                content_list.append({\"type\": \"text\", \"text\": f\"[Unsupported file type: {os.path.basename(task_input)}]\"})\n",
    "\n",
    "        # Handle direct text input\n",
    "        elif isinstance(task_input, str):\n",
    "            print(\"   Processing raw text description.\")\n",
    "            content_list.append({\"type\": \"text\", \"text\": task_input})\n",
    "\n",
    "        # Handle other unexpected input types\n",
    "        else:\n",
    "            error_message = f\"Unexpected input type for task description: {type(task_input).__name__}. Expected text or file path.\"\n",
    "            print(f\"   ⚠️ {error_message}\")\n",
    "\n",
    "\n",
    "        # Final check: if content is empty and no specific error was caught, raise a general one.\n",
    "        if not content_list and not error_message:\n",
    "            error_message = \"Could not process the provided task description input.\"\n",
    "            print(f\"   ⚠️ {error_message}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Unexpected error in load_task_description: {e}\")\n",
    "        error_message = f\"Failed to process task description input: {e}\"\n",
    "\n",
    "    # Return processed content and any accumulated error message\n",
    "    return {\"task_description_content\": content_list, \"error_message\": error_message}\n",
    "\n",
    "def load_user_answer(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Loads the user's answer (text or from .txt file) into plain text.\"\"\"\n",
    "    print(\"--- Loading User Answer ---\")\n",
    "    if state.get(\"error_message\"): return {\"error_message\": state[\"error_message\"]} # Propagate errors\n",
    "\n",
    "    answer_input = state.get('user_answer_input')\n",
    "    answer_text = None\n",
    "    error_message = None\n",
    "\n",
    "    if not answer_input:\n",
    "        return {\"error_message\": \"User answer input missing in state.\"}\n",
    "\n",
    "    try:\n",
    "        # Check if it's a file path\n",
    "        if isinstance(answer_input, str) and os.path.exists(answer_input) and os.path.isfile(answer_input):\n",
    "            print(f\"   Processing file path: {answer_input}\")\n",
    "            file_ext = os.path.splitext(answer_input)[1].lower()\n",
    "            if file_ext == '.txt':\n",
    "                try:\n",
    "                    print(\"   Reading text file...\")\n",
    "                    with open(answer_input, 'r', encoding='utf-8') as f:\n",
    "                        answer_text = f.read()\n",
    "                    if not answer_text or not answer_text.strip():\n",
    "                         error_message = f\"Answer file '{os.path.basename(answer_input)}' is empty or contains only whitespace.\"\n",
    "                         print(f\"   ⚠️ {error_message}\")\n",
    "                         answer_text = None # Treat as no answer provided\n",
    "                    else:\n",
    "                         print(f\"   Loaded answer text from '{os.path.basename(answer_input)}'.\")\n",
    "                except Exception as txt_e:\n",
    "                    print(f\"   ⚠️ Error reading answer file: {txt_e}\")\n",
    "                    error_message = f\"Failed to read answer file '{os.path.basename(answer_input)}': {txt_e}\"\n",
    "            # Add support for other formats like .docx if needed using libraries like 'python-docx'\n",
    "            # elif file_ext == '.docx': ...\n",
    "            else:\n",
    "                error_message = f\"Unsupported file type for answer: '{file_ext}'. Please provide a .txt file or paste the text directly.\"\n",
    "                print(f\"   ⚠️ {error_message}\")\n",
    "\n",
    "        # Handle direct text input\n",
    "        elif isinstance(answer_input, str):\n",
    "            print(\"   Processing raw answer text.\")\n",
    "            if not answer_input.strip():\n",
    "                 error_message = \"Provided answer text is empty or contains only whitespace.\"\n",
    "                 print(f\"   ⚠️ {error_message}\")\n",
    "            else:\n",
    "                 answer_text = answer_input\n",
    "\n",
    "        # Handle other unexpected input types\n",
    "        else:\n",
    "            error_message = f\"Unexpected input type for user answer: {type(answer_input).__name__}. Expected text or file path.\"\n",
    "            print(f\"   ⚠️ {error_message}\")\n",
    "\n",
    "        # If processing failed but resulted in no text and no specific error\n",
    "        if not answer_text and not error_message:\n",
    "             error_message = \"Could not process the provided user answer input.\"\n",
    "             print(f\"   ⚠️ {error_message}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ Unexpected error in load_user_answer: {e}\")\n",
    "        error_message = f\"Failed to process user answer input: {e}\"\n",
    "\n",
    "    # Return loaded text and any error\n",
    "    return {\"user_answer_text\": answer_text, \"error_message\": error_message}\n",
    "\n",
    "print(\"✅ Loading node functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57987498",
   "metadata": {
    "papermill": {
     "duration": 0.006983,
     "end_time": "2025-04-23T11:24:47.787233",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.780250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Task identification and classification\n",
    "Includes the functions that use the standard LLM (llm) to analyze the loaded task description (identify_task_number to determine Task 1 vs Task 2) and then classify the specific type (classify_task_1_type or classify_task_2_type). Contains fallback logic using keywords if the LLM fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "422389fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.804205Z",
     "iopub.status.busy": "2025-04-23T11:24:47.803864Z",
     "iopub.status.idle": "2025-04-23T11:24:47.825130Z",
     "shell.execute_reply": "2025-04-23T11:24:47.824118Z"
    },
    "papermill": {
     "duration": 0.03175,
     "end_time": "2025-04-23T11:24:47.826531",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.794781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Identification and Classification node functions defined.\n"
     ]
    }
   ],
   "source": [
    "def identify_task_number(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Identifies whether the task is Task 1 or Task 2 using LLM and heuristics.\"\"\"\n",
    "    print(\"--- Identifying Task Number ---\")\n",
    "    if state.get(\"error_message\"): return {\"error_message\": state[\"error_message\"]}\n",
    "\n",
    "    description_content = state.get(\"task_description_content\")\n",
    "    if not description_content: return {\"error_message\": \"Task description content missing.\"}\n",
    "\n",
    "    text_parts = [item['text'] for item in description_content if item['type'] == 'text']\n",
    "    description_text = \"\\n\".join(text_parts)\n",
    "    has_image = any(item['type'] == 'image_url' for item in description_content)\n",
    "    print(f\"   Analyzing description (has_image={has_image})...\")\n",
    "\n",
    "    prompt = f\"\"\"Analyze the IELTS task description to determine if it is Task 1 (describes visual info like charts, maps, processes) or Task 2 (essay question).\n",
    "Keywords Task 1: shows, illustrates, depicts, diagram, table, graph, chart, map. Image implies Task 1.\n",
    "Keywords Task 2: discuss, opinion, agree, disagree, advantages, problem, solution, causes, effects, extent.\n",
    "Image provided: {'Yes' if has_image else 'No'}.\n",
    "Description Text:\n",
    "---\n",
    "{description_text}\n",
    "---\n",
    "Respond ONLY with '1' or '2'.\n",
    "\"\"\"\n",
    "    task_number = None\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        task_number_str = response.content.strip().replace(\"'\", \"\").replace(\"\\\"\", \"\")\n",
    "        if task_number_str == '1': task_number = 1\n",
    "        elif task_number_str == '2': task_number = 2\n",
    "        else: print(f\"   LLM classification unclear ('{task_number_str}'). Using fallback.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ LLM Error during task number identification: {e}. Using fallback.\")\n",
    "\n",
    "    # Fallback\n",
    "    if task_number is None:\n",
    "        t1_keywords = ['show', 'illustrate', 'depict', 'diagram', 'table', 'graph', 'chart', 'map', 'plan', 'process']\n",
    "        t2_keywords = ['discuss', 'opinion', 'agree', 'disagree', 'solution', 'cause', 'effect', 'advantage', 'disadvantage', 'extent', 'problem']\n",
    "        desc_lower = description_text.lower()\n",
    "        if has_image: task_number = 1; print(\"   Fallback: Task 1 (Image detected).\")\n",
    "        elif any(kw in desc_lower for kw in t2_keywords): task_number = 2; print(\"   Fallback: Task 2 (Essay keywords detected).\")\n",
    "        elif any(kw in desc_lower for kw in t1_keywords): task_number = 1; print(\"   Fallback: Task 1 (Visual keywords detected).\")\n",
    "        else: task_number = 2; print(\"   Fallback: Defaulting to Task 2.\") # Default assumption\n",
    "\n",
    "    print(f\"   ==> Identified Task Number: {task_number}\")\n",
    "    return {\"task_number\": task_number, \"error_message\": None}\n",
    "\n",
    "def classify_task_1_type(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Classifies the specific type of Task 1.\"\"\"\n",
    "    print(\"--- Classifying Task 1 Type ---\")\n",
    "    if state.get(\"error_message\"): return {\"error_message\": state[\"error_message\"]}\n",
    "    description_content = state.get(\"task_description_content\")\n",
    "    if not description_content: return {\"error_message\": \"Task description content missing.\"}\n",
    "\n",
    "    llm_input_content = []\n",
    "    text_parts = []\n",
    "    has_image = False\n",
    "    for item in description_content:\n",
    "        llm_input_content.append(item)\n",
    "        if item['type'] == 'text': text_parts.append(item['text'])\n",
    "        if item['type'] == 'image_url': has_image = True\n",
    "    prompt_text = \"\\n\".join(text_parts)\n",
    "    print(f\"   Analyzing Task 1 description/visual (has_image={has_image})...\")\n",
    "\n",
    "    classification_prompt = f\"\"\"Analyze the provided IELTS Task 1 description and image (if present). Identify the specific visual type.\n",
    "Description Text: --- {prompt_text} --- Image Provided: {'Yes' if has_image else 'No'}\n",
    "Choose ONE type: 'bar_chart', 'line_graph', 'pie_chart', 'table_chart', 'map', 'process_diagram', 'multiple_charts'. Respond 'unknown' if unclear. Respond ONLY with the type name.\n",
    "\"\"\"\n",
    "    combined_input_for_llm = [{\"type\": \"text\", \"text\": classification_prompt}] + llm_input_content\n",
    "    task_type = None\n",
    "    allowed_types = ['bar_chart', 'line_graph', 'pie_chart', 'table_chart', 'map', 'process_diagram', 'multiple_charts', 'unknown']\n",
    "\n",
    "    try:\n",
    "        llm_to_use = multimodal_llm if has_image else llm\n",
    "        response = llm_to_use.invoke([HumanMessage(content=combined_input_for_llm)])\n",
    "        task_type_str = response.content.strip().lower().replace(\"'\", \"\").replace(\"\\\"\", \"\")\n",
    "        if task_type_str in allowed_types: task_type = task_type_str\n",
    "        else: print(f\"   LLM classification invalid ('{task_type_str}'). Using fallback.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ LLM Error during Task 1 type classification: {e}. Using fallback.\")\n",
    "\n",
    "    # Fallback\n",
    "    if task_type is None:\n",
    "        desc_lower = prompt_text.lower()\n",
    "        if 'process' in desc_lower or 'how something is made' in desc_lower or 'stages of' in desc_lower: task_type = 'process_diagram'\n",
    "        elif 'map' in desc_lower or 'plan of' in desc_lower: task_type = 'map'\n",
    "        elif 'table' in desc_lower: task_type = 'table_chart'\n",
    "        elif 'pie chart' in desc_lower: task_type = 'pie_chart'\n",
    "        elif 'bar chart' in desc_lower or 'bar graph' in desc_lower: task_type = 'bar_chart'\n",
    "        elif 'line graph' in desc_lower or 'line chart' in desc_lower: task_type = 'line_graph'\n",
    "        elif 'charts show' in desc_lower or 'graphs show' in desc_lower or ('chart' in desc_lower and 'graph' in desc_lower): task_type = 'multiple_charts'\n",
    "        else: task_type = 'unknown'\n",
    "        print(f\"   Fallback classification: {task_type}\")\n",
    "\n",
    "    print(f\"   ==> Identified Task 1 Type: {task_type}\")\n",
    "    return {\"task_1_type\": task_type, \"error_message\": None}\n",
    "\n",
    "def classify_task_2_type(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Classifies the specific type of Task 2 essay.\"\"\"\n",
    "    print(\"--- Classifying Task 2 Type ---\")\n",
    "    if state.get(\"error_message\"): return {\"error_message\": state[\"error_message\"]}\n",
    "    description_content = state.get(\"task_description_content\")\n",
    "    if not description_content: return {\"error_message\": \"Task description content missing.\"}\n",
    "\n",
    "    text_parts = [item['text'] for item in description_content if item['type'] == 'text']\n",
    "    description_text = \"\\n\".join(text_parts)\n",
    "    print(f\"   Analyzing Task 2 question: '{description_text[:150]}...'\")\n",
    "\n",
    "    prompt = f\"\"\"Analyze the IELTS Task 2 essay question. Identify the specific type.\n",
    "Question: --- {description_text} ---\n",
    "Types & Keywords:\n",
    "'opinion': agree/disagree, opinion, extent\n",
    "'discussion': discuss both views/sides\n",
    "'problem_solution': causes, problems, solutions, solve\n",
    "'advantages_disadvantages': advantages, disadvantages, outweigh\n",
    "'double_question': Two distinct questions\n",
    "Choose ONE type: 'opinion', 'discussion', 'problem_solution', 'advantages_disadvantages', 'double_question'. Respond 'unknown' if unclear. Respond ONLY with the type name.\n",
    "\"\"\"\n",
    "    task_type = None\n",
    "    allowed_types = ['opinion', 'discussion', 'problem_solution', 'advantages_disadvantages', 'double_question', 'unknown']\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        task_type_str = response.content.strip().lower().replace(\"'\", \"\").replace(\"\\\"\", \"\")\n",
    "        if task_type_str in allowed_types: task_type = task_type_str\n",
    "        else: print(f\"   LLM classification invalid ('{task_type_str}'). Using fallback.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️ LLM Error during Task 2 type classification: {e}. Using fallback.\")\n",
    "\n",
    "    # Fallback\n",
    "    if task_type is None:\n",
    "        desc_lower = description_text.lower()\n",
    "        q_count = description_text.count('?')\n",
    "        if 'to what extent' in desc_lower or 'do you agree or disagree' in desc_lower or \"what is your opinion\" in desc_lower: task_type = 'opinion'\n",
    "        elif 'discuss both' in desc_lower and ('and give your' in desc_lower or 'and give opinion' in desc_lower): task_type = 'discussion'\n",
    "        elif 'advantages' in desc_lower and 'disadvantages' in desc_lower: task_type = 'advantages_disadvantages'\n",
    "        elif ('problem' in desc_lower or 'cause' in desc_lower) and ('solution' in desc_lower or 'solve' in desc_lower): task_type = 'problem_solution'\n",
    "        elif q_count >= 2 and ('why' in desc_lower or 'what' in desc_lower or 'how' in desc_lower): task_type = 'double_question'\n",
    "        else: task_type = 'unknown'\n",
    "        print(f\"   Fallback classification: {task_type}\")\n",
    "\n",
    "    print(f\"   ==> Identified Task 2 Type: {task_type}\")\n",
    "    return {\"task_2_type\": task_type, \"error_message\": None}\n",
    "\n",
    "print(\"✅ Identification and Classification node functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8d373",
   "metadata": {
    "papermill": {
     "duration": 0.007232,
     "end_time": "2025-04-23T11:24:47.841078",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.833846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Summarize graph and evaluation\n",
    "extract_visual_summary function (for Task 1 images) using the multimodal LLM (multimodal_llm) to identify key visual features. Crucially, it contains the main evaluate_writing function, which uses the multimodal LLM, all gathered context (task type, description, answer, rubrics, structure, visual summary), and detailed feedback for each criterion, outputting the results as structured JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb375454",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.859558Z",
     "iopub.status.busy": "2025-04-23T11:24:47.859289Z",
     "iopub.status.idle": "2025-04-23T11:24:47.868893Z",
     "shell.execute_reply": "2025-04-23T11:24:47.867946Z"
    },
    "papermill": {
     "duration": 0.022113,
     "end_time": "2025-04-23T11:24:47.870385",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.848272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Visual Summary node function defined.\n"
     ]
    }
   ],
   "source": [
    "def extract_visual_summary(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Extracts key features from the visual for Task 1 using multimodal LLM.\"\"\"\n",
    "    print(\"--- Extracting Visual Summary (Task 1) ---\")\n",
    "    if state.get(\"error_message\"): return {\"error_message\": state[\"error_message\"]}\n",
    "\n",
    "    description_content = state.get(\"task_description_content\", [])\n",
    "    task_1_type = state.get(\"task_1_type\", \"unknown\")\n",
    "\n",
    "    llm_input_content = []\n",
    "    text_parts = []\n",
    "    has_valid_image = False\n",
    "    image_load_error = None\n",
    "    for item in description_content:\n",
    "        if item['type'] == 'image_url' and item['image_url']['url'].startswith('data:image'):\n",
    "            llm_input_content.append(item)\n",
    "            has_valid_image = True\n",
    "        elif item['type'] == 'text':\n",
    "            llm_input_content.append(item)\n",
    "            text_parts.append(item['text'])\n",
    "            if \"[Error loading image:\" in item['text']: image_load_error = item['text']\n",
    "\n",
    "    prompt_text = \"\\n\".join(text_parts)\n",
    "\n",
    "    if not has_valid_image:\n",
    "         summary = \"No visual provided or visual could not be processed.\"\n",
    "         if image_load_error: summary = f\"Visual summary skipped due to image loading error noted: {image_load_error}\"\n",
    "         print(f\"   Skipping visual summary: {summary}\")\n",
    "         return {\"intermediate_results\": {**state.get(\"intermediate_results\", {}), \"visual_summary\": summary}, \"error_message\": None}\n",
    "\n",
    "    print(f\"   Extracting summary for visual type: {task_1_type}\")\n",
    "    summary_prompt = f\"\"\"As an expert IELTS Task 1 analyst, examine the visual (type: {task_1_type}) and its description.\n",
    "Task Description Text: --- {prompt_text} --- Image Data is Provided.\n",
    "Identify and list the *essential key features, main trends, significant data points, and necessary comparisons* that MUST be included in a high-scoring (Band 7+) Task 1 report. Be specific with values/labels. Do NOT write the report. Output ONLY a bulleted list.\n",
    "Key Features List:\n",
    "\"\"\"\n",
    "    combined_input_for_llm = [{\"type\": \"text\", \"text\": summary_prompt}] + llm_input_content\n",
    "    visual_summary = \"Error: Summary extraction failed.\"\n",
    "    try:\n",
    "        print(\"   Invoking multimodal LLM for summary...\")\n",
    "        response = multimodal_llm.invoke([HumanMessage(content=combined_input_for_llm)])\n",
    "        visual_summary = response.content.strip()\n",
    "        print(f\"   ==> Extracted Visual Summary:\\n{visual_summary}\")\n",
    "    except Exception as e:\n",
    "        visual_summary = f\"Error extracting summary via LLM: {e}\"\n",
    "        print(f\"   ⚠️ LLM Error during visual summary: {e}\")\n",
    "\n",
    "    intermediate_results = {**state.get(\"intermediate_results\", {}), \"visual_summary\": visual_summary}\n",
    "    return {\"intermediate_results\": intermediate_results, \"error_message\": None}\n",
    "\n",
    "print(\"✅ Visual Summary node function defined.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9e831",
   "metadata": {
    "papermill": {
     "duration": 0.006874,
     "end_time": "2025-04-23T11:24:47.884632",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.877758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Helper function for rounding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b740174a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.900679Z",
     "iopub.status.busy": "2025-04-23T11:24:47.900036Z",
     "iopub.status.idle": "2025-04-23T11:24:47.905639Z",
     "shell.execute_reply": "2025-04-23T11:24:47.904363Z"
    },
    "papermill": {
     "duration": 0.015318,
     "end_time": "2025-04-23T11:24:47.907183",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.891865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper function defined.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def round_to_nearest_half(number: float) -> float:\n",
    "    \"\"\"Rounds a number to the nearest 0.5 (e.g., 6.1 -> 6.0, 6.25 -> 6.5, 6.75 -> 7.0).\"\"\"\n",
    "    return round(number * 2) / 2.0\n",
    "\n",
    "print(\"✅ Helper function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75b8bc7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.923495Z",
     "iopub.status.busy": "2025-04-23T11:24:47.923220Z",
     "iopub.status.idle": "2025-04-23T11:24:47.947034Z",
     "shell.execute_reply": "2025-04-23T11:24:47.945957Z"
    },
    "papermill": {
     "duration": 0.033949,
     "end_time": "2025-04-23T11:24:47.948466",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.914517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation node function updated for Text Output parsing.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_writing(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Performs evaluation, asks LLM for text output, parses it robustly, and calculates overall score.\"\"\"\n",
    "    print(\"--- Evaluating Writing ---\")\n",
    "    if state.get(\"error_message\"): return {\"error_message\": state[\"error_message\"]}\n",
    "\n",
    "    # --- Gather context (same as before) ---\n",
    "    task_number = state.get(\"task_number\")\n",
    "    task_1_type = state.get(\"task_1_type\")\n",
    "    task_2_type = state.get(\"task_2_type\")\n",
    "    user_answer = state.get(\"user_answer_text\")\n",
    "    description_content = state.get(\"task_description_content\", [])\n",
    "    visual_summary = state.get(\"intermediate_results\", {}).get(\"visual_summary\")\n",
    "\n",
    "    if not task_number or not user_answer:\n",
    "        return {\"error_message\": \"Cannot evaluate: Missing task number or user answer.\"}\n",
    "\n",
    "    if task_number == 1:\n",
    "        task_type_name = task_1_type or 'unknown'; rubric_key = \"Task 1\"; response_criterion = \"TA\"\n",
    "    else:\n",
    "        task_type_name = task_2_type or 'unknown'; rubric_key = \"Task 2\"; response_criterion = \"TR\"\n",
    "    criteria_names = [response_criterion, \"CC\", \"LR\", \"GRA\"]\n",
    "    print(f\"   Evaluating Task {task_number} ({task_type_name}) answer...\")\n",
    "\n",
    "    try: # Prepare rubrics/structure for prompt\n",
    "        rubrics_for_task = IELTS_RUBRICS[rubric_key]\n",
    "        structure_guideline = TASK_STRUCTURES[rubric_key].get(task_type_name, TASK_STRUCTURES[rubric_key]['unknown'])\n",
    "        rubric_prompt_text = f\"Key points from IELTS {rubric_key} Band Descriptors (Focus on Bands 9, 8, 7, 6, 5):\\n\"\n",
    "        for crit in criteria_names:\n",
    "             rubric_prompt_text += f\"\\n{crit}:\\n\"\n",
    "             for band in [9, 8, 7, 6, 5]:\n",
    "                 desc = rubrics_for_task[crit].get(band, f\"Band {band} N/A\")\n",
    "                 rubric_prompt_text += f\"  Band {band}: {desc[:150].strip()}...\\n\"\n",
    "    except KeyError as e:\n",
    "        return {\"error_message\": f\"Configuration error: Missing rubrics/structure for {e}.\"}\n",
    "\n",
    "    description_text = \"\\n\".join([item['text'] for item in description_content if item['type'] == 'text'])\n",
    "\n",
    "    # --- Evaluation Prompt ---\n",
    "    eval_prompt_parts = [\n",
    "        f\"You are a meticulous, fair, and highly detailed IELTS Writing examiner. Your goal is to provide an accurate band score and exceptionally thorough, constructive feedback.\",\n",
    "        f\"Evaluate the user's answer for IELTS Writing Task {task_number} ({task_type_name}).\",\n",
    "        f\"\\nTask Description Context:\\n---\\n{description_text}\\n---\"\n",
    "    ]\n",
    "    if task_number == 1 and visual_summary and \"Error\" not in visual_summary and \"No visual\" not in visual_summary and \"not be processed\" not in visual_summary:\n",
    "        eval_prompt_parts.append(f\"\\nExpected Key Features (from visual analysis):\\n---\\n{visual_summary}\\n---\")\n",
    "    elif task_number == 1:\n",
    "        eval_prompt_parts.append(f\"\\nNote Regarding Visual: {visual_summary or 'Not generated.'}\\n---\")\n",
    "\n",
    "    eval_prompt_parts.extend([\n",
    "        f\"\\nUser's Answer:\\n---\\n{user_answer}\\n---\",\n",
    "        f\"\\nSuggested Structure Guide for '{task_type_name}':\\n---\\n{structure_guideline}\\n---\",\n",
    "        f\"\\nReference IELTS Band Descriptors ({rubric_key}):\\n---\\n{rubric_prompt_text}\\n---\",\n",
    "        f\"\\nEvaluation Instructions:\",\n",
    "        f\"1. Accuracy is paramount. Evaluate *strictly* based on the IELTS criteria: {response_criterion}, CC, LR, GRA.\",\n",
    "        f\"2. **Assign a score for EACH criterion using the IELTS scale (1.0 to 9.0) in increments of 0.5** (e.g., 6.0, 6.5, 7.0, 7.5).\",\n",
    "        f\"3. **Assign a '.5' score (e.g., 7.5) if performance surpasses the lower band but doesn't consistently meet the next whole band.**\",\n",
    "        f\"4. **Provide VERY DETAILED, SPECIFIC, and CONSTRUCTIVE feedback for EACH criterion.**\",\n",
    "        f\"    - Justify the score with clear explanations referencing the band descriptors.\",\n",
    "        f\"    - **Cite specific examples or patterns from the user's text.** Use brief quotes if helpful (e.g., 'The phrase \\\"increaseMENT\\\" shows a word formation error.').\",\n",
    "        f\"    - Explicitly list **Strengths** (what the user did well according to the criteria).\",\n",
    "        f\"    - Explicitly list **Weaknesses/Areas for Improvement** (specific issues and *actionable* advice on how to improve). Be precise (e.g., 'Improve topic sentences by...', 'Vary sentence structures by including complex sentences like...', 'Check for subject-verb agreement errors such as...').\",\n",
    "        f\"    - Ensure feedback addresses all aspects of the criterion (e.g., for CC: organization, paragraphing, cohesion; for LR: range, accuracy, collocation; for GRA: range, accuracy, punctuation).\",\n",
    "        f\"    - For {response_criterion}: Be specific about which task requirements were met/missed (e.g., 'Key feature X was well-described, but the comparison between Y and Z was missing.' or 'The first part of the question was addressed, but the second part lacked sufficient development.').\",\n",
    "        f\"5. **Output the results as plain text, following this EXACT format for EACH criterion:**\",\n",
    "        f\"--- CRITERION: {response_criterion} ---\",\n",
    "        f\"Score: [SCORE_FLOAT_1.0_to_9.0_in_0.5_steps]\",\n",
    "        f\"Feedback:\",\n",
    "        f\"Strengths: [List specific strengths here, citing examples/patterns]\",\n",
    "        f\"Weaknesses/Areas for Improvement: [List specific weaknesses and actionable advice, citing examples/patterns]\",\n",
    "        f\"--- CRITERION: CC ---\",\n",
    "        f\"Score: [SCORE_FLOAT_1.0_to_9.0_in_0.5_steps]\",\n",
    "        f\"Feedback:\",\n",
    "        f\"Strengths: [List specific strengths here]\",\n",
    "        f\"Weaknesses/Areas for Improvement: [List specific weaknesses and actionable advice]\",\n",
    "        f\"--- CRITERION: LR ---\",\n",
    "        f\"Score: [SCORE_FLOAT_1.0_to_9.0_in_0.5_steps]\",\n",
    "        f\"Feedback:\",\n",
    "        f\"Strengths: [List specific strengths here]\",\n",
    "        f\"Weaknesses/Areas for Improvement: [List specific weaknesses and actionable advice]\",\n",
    "        f\"--- CRITERION: GRA ---\",\n",
    "        f\"Score: [SCORE_FLOAT_1.0_to_9.0_in_0.5_steps]\",\n",
    "        f\"Feedback:\",\n",
    "        f\"Strengths: [List specific strengths here]\",\n",
    "        f\"Weaknesses/Areas for Improvement: [List specific weaknesses and actionable advice]\",\n",
    "        f\"--- END EVALUATION ---\",\n",
    "        f\"6. **Do NOT include any other text, apologies, or formatting.** Ensure feedback under each criterion is substantial.\"\n",
    "    ])\n",
    "\n",
    "    \n",
    "    final_eval_prompt = \"\\n\".join(eval_prompt_parts)\n",
    "    llm_eval_input_messages = [HumanMessage(content=description_content + [{\"type\": \"text\", \"text\": final_eval_prompt}])]\n",
    "    eval_llm = multimodal_llm if any(item.get('type') == 'image_url' for item in description_content) else llm\n",
    "\n",
    "    parsed_evaluation_criteria = {}\n",
    "    calculated_overall_score = None\n",
    "    error_message = None\n",
    "    possible_scores_set = {i / 2.0 for i in range(2, 19)} # 1.0, 1.5, ..., 9.0\n",
    "\n",
    "    try:\n",
    "        print(\"   Invoking LLM for evaluation (text output)...\")\n",
    "        # Increase timeout slightly more if needed, evaluation can be long\n",
    "        # eval_llm.request_options = {\"timeout\": 360}\n",
    "        response = eval_llm.invoke(llm_eval_input_messages)\n",
    "        response_content = response.content.strip()\n",
    "        print(\"   LLM evaluation response received.\")\n",
    "        print(f\"   Raw LLM Response (up to 2000 chars):\\n---\\n{response_content[:2000]}\\n---\")\n",
    "\n",
    "        # --- ROBUST Text Parsing Logic (with NameError fix) ---\n",
    "        current_criterion = None\n",
    "        current_score = None # Use this variable to hold score for the current section\n",
    "        current_feedback_lines = []\n",
    "\n",
    "        response_to_parse = response_content + \"\\n--- END EVALUATION ---\" # Append end marker\n",
    "\n",
    "        for line in response_to_parse.splitlines():\n",
    "            line_stripped = line.strip()\n",
    "\n",
    "            # Check for new criterion marker OR end marker\n",
    "            is_marker = line_stripped.startswith(\"--- CRITERION:\")\n",
    "            is_end_marker = line_stripped.startswith(\"--- END EVALUATION ---\")\n",
    "\n",
    "            if is_marker or is_end_marker:\n",
    "                # Finalize the PREVIOUS criterion before starting new or ending\n",
    "                if current_criterion and current_criterion in criteria_names:\n",
    "                    feedback_text = \"\\n\".join(current_feedback_lines).strip()\n",
    "                    # *** FIX: Check current_score, not score ***\n",
    "                    if current_score is None:\n",
    "                         print(f\"   ⚠️ Critical Error: Score not found/parsed for {current_criterion}.\")\n",
    "                         # Set score to a placeholder like 0 or None to indicate failure\n",
    "                         parsed_evaluation_criteria[current_criterion] = {\"score\": None, \"feedback\": feedback_text or \"(Feedback extracted, but score parsing failed)\"}\n",
    "                    elif not feedback_text:\n",
    "                         print(f\"   ⚠️ Warning: Feedback seems empty for {current_criterion}.\")\n",
    "                         parsed_evaluation_criteria[current_criterion] = {\"score\": current_score, \"feedback\": \"(Feedback not extracted or provided by LLM)\"}\n",
    "                    else:\n",
    "                        # Store valid parsed data\n",
    "                        parsed_evaluation_criteria[current_criterion] = {\n",
    "                            \"score\": current_score,\n",
    "                            \"feedback\": feedback_text\n",
    "                        }\n",
    "                    # Reset for next criterion\n",
    "                    current_score = None\n",
    "                    current_feedback_lines = []\n",
    "\n",
    "                # Extract the new criterion name (if it's a criterion marker)\n",
    "                if is_marker:\n",
    "                    current_criterion = line_stripped.replace(\"--- CRITERION:\", \"\").replace(\"---\", \"\").strip()\n",
    "                    if current_criterion not in criteria_names:\n",
    "                         print(f\"   ⚠️ Warning: Found unexpected criterion marker '{current_criterion}'.\")\n",
    "                         current_criterion = None # Ignore this section\n",
    "                    else:\n",
    "                         print(f\"   Parsing section for: {current_criterion}\")\n",
    "                else: # It's the end marker\n",
    "                    current_criterion = None\n",
    "                continue # Move to the next line\n",
    "\n",
    "            # If we are inside a valid criterion section being processed\n",
    "            if current_criterion:\n",
    "                line_lower_stripped = line_stripped.lower()\n",
    "                # Find score line (only once per section)\n",
    "                if current_score is None and line_lower_stripped.startswith(\"score:\"):\n",
    "                    try:\n",
    "                        score_str = line_stripped.split(\":\", 1)[1].strip()\n",
    "                        parsed_score = float(score_str)\n",
    "                        if parsed_score in possible_scores_set:\n",
    "                            current_score = parsed_score # Store the valid score\n",
    "                            print(f\"      Found score: {current_score}\")\n",
    "                        else:\n",
    "                            print(f\"   ⚠️ Warning: Invalid score value '{parsed_score}' for {current_criterion}.\")\n",
    "                    except Exception as score_e:\n",
    "                        print(f\"   ⚠️ Error parsing score for {current_criterion} from '{line_stripped}': {score_e}\")\n",
    "                    continue # Don't add score line to feedback\n",
    "\n",
    "                # Skip the literal \"Feedback:\" line\n",
    "                elif line_lower_stripped == \"feedback:\":\n",
    "                    continue\n",
    "\n",
    "                # Otherwise, add to feedback lines\n",
    "                elif line_stripped: # Add non-empty lines\n",
    "                    current_feedback_lines.append(line_stripped) # Collect feedback lines\n",
    "\n",
    "        # --- Validation and Score Calculation ---\n",
    "        num_parsed = len(parsed_evaluation_criteria)\n",
    "        print(f\"   Finished parsing. Found data for {num_parsed} criteria sections.\")\n",
    "        if num_parsed != 4:\n",
    "            missing = [c for c in criteria_names if c not in parsed_evaluation_criteria]\n",
    "            error_message = f\"Parsing Error: Expected 4 criteria, but only found {num_parsed}. Missing or failed to parse: {missing}.\"\n",
    "            print(f\"   ⚠️ {error_message}\")\n",
    "        else:\n",
    "            valid_scores = []\n",
    "            failed_scores = []\n",
    "            for crit, data in parsed_evaluation_criteria.items():\n",
    "                if data.get('score') is not None: valid_scores.append(data['score'])\n",
    "                else: failed_scores.append(crit)\n",
    "\n",
    "            if failed_scores:\n",
    "                error_message = f\"Parsing Error: Failed to extract valid scores for criteria: {failed_scores}.\"\n",
    "                print(f\"   ⚠️ {error_message}\")\n",
    "            else:\n",
    "                avg_score = sum(valid_scores) / 4.0\n",
    "                calculated_overall_score = round_to_nearest_half(avg_score)\n",
    "                print(f\"   Evaluation parsed successfully. Calculated Overall Score: {calculated_overall_score}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error during evaluation processing: {e}\"\n",
    "        print(f\"   ⚠️ {error_message}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    if calculated_overall_score is not None and not error_message:\n",
    "        return { \"evaluation_criteria\": parsed_evaluation_criteria, \"overall_score\": calculated_overall_score, \"error_message\": None }\n",
    "    else:\n",
    "         final_error = error_message or \"Evaluation failed: Could not parse all results.\"\n",
    "         return {\"evaluation_criteria\": parsed_evaluation_criteria, \"error_message\": final_error} # Pass partial results if error occurred late\n",
    "\n",
    "print(\"✅ Evaluation node function updated for Text Output parsing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617705d",
   "metadata": {
    "papermill": {
     "duration": 0.007022,
     "end_time": "2025-04-23T11:24:47.963255",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.956233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Report compilation and error handling\n",
    "Includes compile_final_report which takes the JSON evaluation results from the state and formats them into a user-friendly Markdown report. It also defines handle_error, a safety-net node that catches errors from any previous step and produces a simple error message report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77a1c6da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:47.979190Z",
     "iopub.status.busy": "2025-04-23T11:24:47.978818Z",
     "iopub.status.idle": "2025-04-23T11:24:47.989576Z",
     "shell.execute_reply": "2025-04-23T11:24:47.988504Z"
    },
    "papermill": {
     "duration": 0.020793,
     "end_time": "2025-04-23T11:24:47.991253",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.970460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_error(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Catches errors and formats an error report.\"\"\"\n",
    "    # print(f\"--- Error Handler Node ---\") # Suppressed\n",
    "    error_message = state.get(\"error_message\", \"Unknown error.\")\n",
    "    # print(f\"   Error caught: {error_message}\") # Suppressed\n",
    "    final_report = f\"## IELTS Writing Grading Failed\\n\\nAn error occurred:\\n\\n```\\n{error_message}\\n```\\n\\nPlease review inputs or check logs.\"\n",
    "    return {\"final_feedback_report\": final_report, \"error_message\": error_message}\n",
    "\n",
    "\n",
    "def compile_final_report(state: IeltsGraderState) -> Dict[str, Any]:\n",
    "    \"\"\"Compiles the final report using full criterion names.\"\"\"\n",
    "    # print(\"--- Compiling Final Report ---\") # Suppressed\n",
    "    error_message = state.get(\"error_message\")\n",
    "    if error_message:\n",
    "        # print(f\"   Skipping report due to previous error: {error_message}\") # Suppressed\n",
    "        # Use the error handling node's output instead if an error occurred earlier\n",
    "        # This node assumes successful evaluation data is present if reached via normal flow\n",
    "         final_report = f\"## Grading Incomplete\\n\\nAn error occurred before final report compilation:\\n```\\n{error_message}\\n```\"\n",
    "         return {\"final_feedback_report\": final_report}\n",
    "\n",
    "\n",
    "    evaluation = state.get(\"evaluation_criteria\")\n",
    "    overall_score = state.get(\"overall_score\")\n",
    "    task_number = state.get(\"task_number\")\n",
    "    task_1_type = state.get(\"task_1_type\")\n",
    "    task_2_type = state.get(\"task_2_type\")\n",
    "\n",
    "    if not evaluation or overall_score is None or not task_number:\n",
    "        err = \"Cannot compile report: Essential evaluation results missing.\"\n",
    "        # print(f\"   ⚠️ {err}\") # Suppressed\n",
    "        return {\"final_feedback_report\": f\"## Grading Incomplete\\n\\n{err}\"}\n",
    "\n",
    "    criterion_full_names = {\n",
    "        \"TA\": \"Task Achievement\",\n",
    "        \"TR\": \"Task Response\",\n",
    "        \"CC\": \"Coherence and Cohesion\",\n",
    "        \"LR\": \"Lexical Resource\",\n",
    "        \"GRA\": \"Grammatical Range and Accuracy\"\n",
    "    }\n",
    "\n",
    "    task_type_name = task_1_type if task_number == 1 else task_2_type\n",
    "    task_type_name = (task_type_name or f\"Task {task_number}\").replace('_', ' ').title()\n",
    "    response_criterion_abbr = \"TA\" if task_number == 1 else \"TR\"\n",
    "    criteria_order = [response_criterion_abbr, \"CC\", \"LR\", \"GRA\"]\n",
    "\n",
    "    report_parts = [\n",
    "        f\"# IELTS Writing Task {task_number} ({task_type_name}) - Feedback Report\",\n",
    "        f\"\\n## Overall Estimated Band Score: {overall_score}\\n\"\n",
    "        f\"{'-'*40}\"\n",
    "    ]\n",
    "\n",
    "    all_criteria_present = True\n",
    "    for crit_abbr in criteria_order:\n",
    "        crit_data = evaluation.get(crit_abbr)\n",
    "        crit_full_name = criterion_full_names.get(crit_abbr, crit_abbr)\n",
    "\n",
    "        if isinstance(crit_data, dict):\n",
    "            score = crit_data.get(\"score\")\n",
    "            feedback = crit_data.get(\"feedback\", \"_No specific feedback provided._\").strip()\n",
    "            score_display = score if score is not None else \"N/A (Parsing Error)\"\n",
    "            report_parts.append(f\"\\n### {crit_full_name} (Score: {score_display})\\n\")\n",
    "            report_parts.append(f\"{feedback}\\n\")\n",
    "            if score is None: all_criteria_present = False\n",
    "        else:\n",
    "             all_criteria_present = False\n",
    "             # Use full name in the error message\n",
    "             report_parts.append(f\"\\n### {crit_full_name} (Score: N/A)\\n\")\n",
    "             report_parts.append(\"_Evaluation data missing for this criterion._\\n\")\n",
    "\n",
    "    # if not all_criteria_present: print(\"   Warning: Some criteria data missing/parsing failed.\") # Suppressed\n",
    "\n",
    "    final_report = \"\\n\".join(report_parts)\n",
    "    # print(\"   ✅ Final report compiled.\") # Suppressed\n",
    "    return {\"final_feedback_report\": final_report, \"error_message\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1008e",
   "metadata": {
    "papermill": {
     "duration": 0.007204,
     "end_time": "2025-04-23T11:24:48.006093",
     "exception": false,
     "start_time": "2025-04-23T11:24:47.998889",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Graph definition\n",
    "This section uses LangGraph (StateGraph) to connect all the previously defined node functions into a coherent workflow. It sets the entry point (ask_for_task_description) and defines the conditional logic (using functions like check_for_errors, decide_task_path) that directs the flow of information and control between nodes based on the current state (e.g., routing to Task 1 vs Task 2 path, handling errors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "746fab0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:48.022851Z",
     "iopub.status.busy": "2025-04-23T11:24:48.022194Z",
     "iopub.status.idle": "2025-04-23T11:24:48.071776Z",
     "shell.execute_reply": "2025-04-23T11:24:48.070794Z"
    },
    "papermill": {
     "duration": 0.059823,
     "end_time": "2025-04-23T11:24:48.073492",
     "exception": false,
     "start_time": "2025-04-23T11:24:48.013669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Defining LangGraph Workflow ---\n",
      "   Adding nodes...\n",
      "   Entry point set.\n",
      "   Adding edges...\n",
      "   Edges defined.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Defining LangGraph Workflow ---\")\n",
    "workflow = StateGraph(IeltsGraderState)\n",
    "\n",
    "# Add nodes\n",
    "print(\"   Adding nodes...\")\n",
    "# (Keep the workflow.add_node(...) calls from the previous version here)\n",
    "workflow.add_node(\"ask_for_task_description\", ask_for_task_description)\n",
    "workflow.add_node(\"load_task_description\", load_task_description)\n",
    "workflow.add_node(\"ask_for_user_answer\", ask_for_user_answer)\n",
    "workflow.add_node(\"load_user_answer\", load_user_answer)\n",
    "workflow.add_node(\"identify_task_number\", identify_task_number)\n",
    "workflow.add_node(\"classify_task_1_type\", classify_task_1_type)\n",
    "workflow.add_node(\"classify_task_2_type\", classify_task_2_type)\n",
    "workflow.add_node(\"extract_visual_summary\", extract_visual_summary)\n",
    "workflow.add_node(\"evaluate_writing\", evaluate_writing)\n",
    "workflow.add_node(\"compile_final_report\", compile_final_report)\n",
    "workflow.add_node(\"handle_error\", handle_error)\n",
    "\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"ask_for_task_description\")\n",
    "print(\"   Entry point set.\")\n",
    "\n",
    "# Conditional routing functions (remain the same logic)\n",
    "def check_for_errors(state: IeltsGraderState) -> Literal[\"error\", \"continue\"]:\n",
    "    return \"error\" if state.get(\"error_message\") else \"continue\"\n",
    "\n",
    "def decide_task_path(state: IeltsGraderState) -> Literal[\"error\", \"task_1_branch\", \"task_2_branch\"]:\n",
    "    if state.get(\"error_message\"): return \"error\"\n",
    "    task_num = state.get(\"task_number\")\n",
    "    if task_num == 1: return \"task_1_branch\"\n",
    "    if task_num == 2: return \"task_2_branch\"\n",
    "    print(\"   Warning: Task number invalid for routing.\")\n",
    "    state['error_message'] = state.get('error_message', '') + \" Task number identification failed for routing.\"\n",
    "    return \"error\"\n",
    "\n",
    "def decide_task_1_visual_path(state: IeltsGraderState) -> Literal[\"error\", \"summarize_visual\", \"evaluate\"]:\n",
    "    if state.get(\"error_message\"): return \"error\"\n",
    "    has_image_data = any(item.get('type') == 'image_url' and item['image_url']['url'].startswith('data:image')\n",
    "                         for item in state.get(\"task_description_content\", []))\n",
    "    task_1_type = state.get(\"task_1_type\")\n",
    "    if has_image_data and task_1_type != 'unknown':\n",
    "        print(\"   Routing Task 1 to: extract_visual_summary\")\n",
    "        return \"summarize_visual\"\n",
    "    else:\n",
    "        print(\"   Routing Task 1 directly to: evaluate_writing\")\n",
    "        return \"evaluate\"\n",
    "\n",
    "# Add edges\n",
    "print(\"   Adding edges...\")\n",
    "workflow.add_conditional_edges(\"ask_for_task_description\", check_for_errors, {\"continue\": \"load_task_description\", \"error\": \"handle_error\"})\n",
    "workflow.add_conditional_edges(\"load_task_description\", check_for_errors, {\"continue\": \"ask_for_user_answer\", \"error\": \"handle_error\"})\n",
    "workflow.add_conditional_edges(\"ask_for_user_answer\", check_for_errors, {\"continue\": \"load_user_answer\", \"error\": \"handle_error\"})\n",
    "workflow.add_conditional_edges(\"load_user_answer\", check_for_errors, {\"continue\": \"identify_task_number\", \"error\": \"handle_error\"})\n",
    "workflow.add_conditional_edges(\"identify_task_number\", decide_task_path, {\"task_1_branch\": \"classify_task_1_type\", \"task_2_branch\": \"classify_task_2_type\", \"error\": \"handle_error\"})\n",
    "workflow.add_conditional_edges(\"classify_task_1_type\", decide_task_1_visual_path, {\"summarize_visual\": \"extract_visual_summary\", \"evaluate\": \"evaluate_writing\", \"error\": \"handle_error\"})\n",
    "workflow.add_conditional_edges(\"classify_task_2_type\", check_for_errors, {\"continue\": \"evaluate_writing\", \"error\": \"handle_error\"})\n",
    "workflow.add_conditional_edges(\"extract_visual_summary\", check_for_errors, {\"continue\": \"evaluate_writing\", \"error\": \"handle_error\"})\n",
    "workflow.add_conditional_edges(\"evaluate_writing\", check_for_errors, {\"continue\": \"compile_final_report\", \"error\": \"handle_error\"})\n",
    "workflow.add_edge(\"compile_final_report\", END)\n",
    "workflow.add_edge(\"handle_error\", END)\n",
    "\n",
    "print(\"   Edges defined.\")\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e7b40b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:24:48.090496Z",
     "iopub.status.busy": "2025-04-23T11:24:48.089671Z",
     "iopub.status.idle": "2025-04-23T11:24:48.105782Z",
     "shell.execute_reply": "2025-04-23T11:24:48.104613Z"
    },
    "papermill": {
     "duration": 0.026173,
     "end_time": "2025-04-23T11:24:48.107229",
     "exception": false,
     "start_time": "2025-04-23T11:24:48.081056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🚀 Starting Interactive IELTS Grader 🚀\n",
      "============================================================\n",
      "ℹ️ You will be prompted for Task Description and Answer.\n",
      "   Provide text directly or the FULL path to a file (Image/PDF/TXT).\n",
      "\n",
      "--- Task Description Input ---\n",
      "\n",
      "============================================================\n",
      "🏁 Grading Process Finished 🏁\n",
      "============================================================\n",
      "\n",
      "## IELTS Writing Grading Failed\n",
      "\n",
      "An error occurred:\n",
      "\n",
      "```\n",
      "Error getting task description input: raw_input was called, but this frontend does not support input requests.\n",
      "```\n",
      "\n",
      "Please review inputs or check logs.\n",
      "\n",
      "--- Script execution attempt complete ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚀 Starting Interactive IELTS Grader 🚀\")\n",
    "print(\"=\"*60)\n",
    "print(\"ℹ️ You will be prompted for Task Description and Answer.\")\n",
    "print(\"   Provide text directly or the FULL path to a file (Image/PDF/TXT).\")\n",
    "\n",
    "# Define the initial empty state for the graph\n",
    "initial_state = IeltsGraderState(\n",
    "    task_description_input=None, user_answer_input=None,\n",
    "    task_description_content=None, user_answer_text=None,\n",
    "    task_number=None, task_1_type=None, task_2_type=None,\n",
    "    intermediate_results={}, evaluation_criteria=None,\n",
    "    overall_score=None, final_feedback_report=None, error_message=None\n",
    ")\n",
    "\n",
    "# Configuration for the graph execution\n",
    "config = {\"recursion_limit\": 25} # Max steps allowed in the graph\n",
    "\n",
    "# === Run the Agent ===\n",
    "# This call starts the graph and will pause for user input()\n",
    "final_state = None\n",
    "try:\n",
    "    # The main execution call\n",
    "    final_state = app.invoke(initial_state, config=config)\n",
    "\n",
    "    # --- Display Final Report ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🏁 Grading Process Finished 🏁\")\n",
    "    print(\"=\"*60)\n",
    "    # Retrieve and print the final report (or error message)\n",
    "    # Use a default message if the key is somehow missing\n",
    "    report = final_state.get(\"final_feedback_report\", \"Error: Final state incomplete, no report generated.\")\n",
    "    print(\"\\n\" + report) # Print the Markdown formatted report\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n❌ Process interrupted by user (Ctrl+C).\")\n",
    "except Exception as e:\n",
    "     # Catch unexpected errors during the execution flow\n",
    "     print(f\"\\n\\n❌ An unexpected error occurred during graph execution: {e}\")\n",
    "     import traceback\n",
    "     traceback.print_exc() # Print detailed traceback for debugging\n",
    "     # Optionally print the state at the time of error\n",
    "     if final_state:\n",
    "        print(\"\\n--- State at time of error ---\")\n",
    "        # Avoid printing large potentially sensitive data like encoded images\n",
    "        state_summary = {k: (type(v).__name__ + (f\" len={len(v)}\" if hasattr(v, '__len__') and k != 'task_description_content' else ''))\n",
    "                         for k, v in final_state.items()}\n",
    "        try:\n",
    "             print(json.dumps(state_summary, indent=2))\n",
    "        except TypeError:\n",
    "             print(str(state_summary)) # Fallback to string\n",
    "\n",
    "# === End of Invocation ===\n",
    "print(\"\\n--- Script execution attempt complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 40.412022,
   "end_time": "2025-04-23T11:24:48.937461",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-23T11:24:08.525439",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
